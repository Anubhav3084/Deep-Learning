{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Recognition.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9SMP3lwlTe0WAtuTiv07X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anubhav3084/Deep-Learning/blob/main/Image_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kvfiF1DzX0a2"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "MRQb9hVWaL4q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coAIViILaZgA",
        "outputId": "bc389385-8bc7-4db1-acf8-546e7f66d135"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ILdebmSapy_",
        "outputId": "51e0a8f3-5cbc-4338-8718-2caa36f0cea9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbAGZYSNar6T",
        "outputId": "46850cd1-c37c-4ca5-9905-6081fd4c92b0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_class_names = {\n",
        "    0: \"Plane\",\n",
        "    1: \"Car\",\n",
        "    2: \"Bird\",\n",
        "    3: \"Cat\",\n",
        "    4: \"Deer\",\n",
        "    5: \"Dog\",\n",
        "    6: \"Frog\",\n",
        "    7: \"Horse\",\n",
        "    8: \"Boat\",\n",
        "    9: \"Truck\"\n",
        "}"
      ],
      "metadata": {
        "id": "fXr5s5Rxa4aW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Normalize data set to 0-to-1 range"
      ],
      "metadata": {
        "id": "zTL-EXwZbxh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype(\"float32\")\n",
        "x_test = x_test.astype(\"float32\")\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255"
      ],
      "metadata": {
        "id": "deY-pSN8bppo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Convert class vectors to binary class matrix"
      ],
      "metadata": {
        "id": "EXy2Pl4gcOm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "UGZ-J0RRcsCR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "dAcB6keDcNzN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSBU8mImccjy",
        "outputId": "eb68474c-fd66-4979-ab28-6374a1649ea7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Model**"
      ],
      "metadata": {
        "id": "k_XSLg4FdGfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D"
      ],
      "metadata": {
        "id": "CVcEiiDgc5Vn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()"
      ],
      "metadata": {
        "id": "NLp8jCmjdXys"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "\n",
        "model.add(MaxPool2D(2, 2))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "\n",
        "model.add(MaxPool2D(2, 2))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(10, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "O263MVjXdZ3G"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elZsLn0ZfQIZ",
        "outputId": "42ae9fd9-355e-44ee-c9c4-fc7028daddac"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 30, 30, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 15, 15, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 15, 15, 32)        0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 15, 15, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 13, 13, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 6, 6, 64)          0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               1180160   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,250,858\n",
            "Trainable params: 1,250,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Compile the model**"
      ],
      "metadata": {
        "id": "ShKhPh6Vh3G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "E_uxwgaPfSne"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training and Saving the model**"
      ],
      "metadata": {
        "id": "ZLNryX79i9M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x_train, \n",
        "    y_train, \n",
        "    batch_size=64, \n",
        "    epochs=30,  # for large dataset we use low epochs\n",
        "    validation_data=(x_test, y_test),\n",
        "    shuffle=True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM3ALEiQieE7",
        "outputId": "f52db158-2b6b-48a5-c8c7-3c3bdb5dc29d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "782/782 [==============================] - 18s 9ms/step - loss: 1.5712 - accuracy: 0.4227 - val_loss: 1.2265 - val_accuracy: 0.5533\n",
            "Epoch 2/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.1765 - accuracy: 0.5820 - val_loss: 1.0428 - val_accuracy: 0.6335\n",
            "Epoch 3/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.0015 - accuracy: 0.6467 - val_loss: 0.8704 - val_accuracy: 0.6961\n",
            "Epoch 4/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.8871 - accuracy: 0.6879 - val_loss: 0.8005 - val_accuracy: 0.7219\n",
            "Epoch 5/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.8228 - accuracy: 0.7139 - val_loss: 0.7482 - val_accuracy: 0.7373\n",
            "Epoch 6/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.7671 - accuracy: 0.7316 - val_loss: 0.7062 - val_accuracy: 0.7564\n",
            "Epoch 7/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.7206 - accuracy: 0.7467 - val_loss: 0.7052 - val_accuracy: 0.7590\n",
            "Epoch 8/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.6961 - accuracy: 0.7539 - val_loss: 0.6869 - val_accuracy: 0.7644\n",
            "Epoch 9/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.6586 - accuracy: 0.7658 - val_loss: 0.7065 - val_accuracy: 0.7617\n",
            "Epoch 10/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.6336 - accuracy: 0.7793 - val_loss: 0.6800 - val_accuracy: 0.7690\n",
            "Epoch 11/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.6056 - accuracy: 0.7875 - val_loss: 0.6491 - val_accuracy: 0.7799\n",
            "Epoch 12/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.5805 - accuracy: 0.7956 - val_loss: 0.6273 - val_accuracy: 0.7857\n",
            "Epoch 13/30\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.5684 - accuracy: 0.7990 - val_loss: 0.6476 - val_accuracy: 0.7823\n",
            "Epoch 14/30\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.5427 - accuracy: 0.8093 - val_loss: 0.6480 - val_accuracy: 0.7783\n",
            "Epoch 15/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.5355 - accuracy: 0.8083 - val_loss: 0.6812 - val_accuracy: 0.7757\n",
            "Epoch 16/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.5164 - accuracy: 0.8171 - val_loss: 0.6253 - val_accuracy: 0.7935\n",
            "Epoch 17/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.5036 - accuracy: 0.8220 - val_loss: 0.6356 - val_accuracy: 0.7909\n",
            "Epoch 18/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4913 - accuracy: 0.8264 - val_loss: 0.6214 - val_accuracy: 0.7940\n",
            "Epoch 19/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4731 - accuracy: 0.8321 - val_loss: 0.6107 - val_accuracy: 0.7934\n",
            "Epoch 20/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4657 - accuracy: 0.8340 - val_loss: 0.6146 - val_accuracy: 0.7958\n",
            "Epoch 21/30\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.4574 - accuracy: 0.8375 - val_loss: 0.6123 - val_accuracy: 0.7992\n",
            "Epoch 22/30\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.4424 - accuracy: 0.8440 - val_loss: 0.6577 - val_accuracy: 0.7869\n",
            "Epoch 23/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4346 - accuracy: 0.8458 - val_loss: 0.6272 - val_accuracy: 0.8001\n",
            "Epoch 24/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4269 - accuracy: 0.8491 - val_loss: 0.6428 - val_accuracy: 0.7979\n",
            "Epoch 25/30\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.4185 - accuracy: 0.8522 - val_loss: 0.6200 - val_accuracy: 0.7991\n",
            "Epoch 26/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4097 - accuracy: 0.8541 - val_loss: 0.6353 - val_accuracy: 0.7992\n",
            "Epoch 27/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4137 - accuracy: 0.8529 - val_loss: 0.6492 - val_accuracy: 0.7983\n",
            "Epoch 28/30\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3993 - accuracy: 0.8587 - val_loss: 0.6185 - val_accuracy: 0.8041\n",
            "Epoch 29/30\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.3905 - accuracy: 0.8613 - val_loss: 0.6328 - val_accuracy: 0.7996\n",
            "Epoch 30/30\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.3841 - accuracy: 0.8626 - val_loss: 0.6330 - val_accuracy: 0.7996\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f34546fa450>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "uSelzBjUkxCV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save neural network structure\n",
        "\n",
        "model_structure = model.to_json()\n",
        "f = Path(\"model_structure.json\")\n",
        "f.write_text(model_structure)\n",
        "\n",
        "# save trained weights\n",
        "\n",
        "model.save_weights(\"model_weights.h5\")"
      ],
      "metadata": {
        "id": "7C4TlAydj3q2"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predicting using trained model**"
      ],
      "metadata": {
        "id": "K87RJLh1mrsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = Path(\"model_structure.json\")\n",
        "model_structure_2 = f.read_text()"
      ],
      "metadata": {
        "id": "K85pxAg7mIlr"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json"
      ],
      "metadata": {
        "id": "BiUWGby9nRtL"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = model_from_json(model_structure_2)"
      ],
      "metadata": {
        "id": "_pE-DET9nD_X"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.load_weights(\"model_weights.h5\")"
      ],
      "metadata": {
        "id": "h5gdoYIvnJ7O"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Vw5W5s4knatU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = image.load_img(\"cat.png\", target_size=(32, 32)) # load image with the target size as 32 x 32 pixels"
      ],
      "metadata": {
        "id": "x1CXrJP2oXFx"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_to_test = image.img_to_array(img) / 255   # convert to numpy array and normalize it to 0-to-1"
      ],
      "metadata": {
        "id": "bXQpM3rvohJG"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add a dimension for the batch size (batch_size, dimensions_of_image)\n",
        "list_of_images = np.expand_dims(image_to_test, axis=0)"
      ],
      "metadata": {
        "id": "MbHyG1jConDS"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.predict(list_of_images)"
      ],
      "metadata": {
        "id": "zgREPYX3pS68"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMu4v3WkpW95",
        "outputId": "a587c806-330d-4a03-cc63-8140496399ee"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.4843871e-08, 1.7341864e-12, 7.7713412e-05, 9.9708956e-01,\n",
              "        1.5435751e-03, 1.2376400e-03, 5.6273939e-06, 4.5913090e-05,\n",
              "        3.5463691e-11, 1.5629902e-09]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "single_result = results[0]\n",
        "single_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08yq2GIDpcXI",
        "outputId": "d679ffb2-56a8-4dee-af59-2c154c61b4a0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.4843871e-08, 1.7341864e-12, 7.7713412e-05, 9.9708956e-01,\n",
              "       1.5435751e-03, 1.2376400e-03, 5.6273939e-06, 4.5913090e-05,\n",
              "       3.5463691e-11, 1.5629902e-09], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_likely_class_index = int(np.argmax(single_result))\n",
        "class_likelihood = single_result[most_likely_class_index]"
      ],
      "metadata": {
        "id": "RFZ8WQL2pgHB"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_label = cifar10_class_names[most_likely_class_index]\n",
        "print(\"This image is predicted as : {} with a likelihood of : {:2f}\".format(class_label, class_likelihood))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0paHQswqqDB2",
        "outputId": "93410ebc-a60a-4938-b161-dd86e1437d88"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image is predicted as : Cat with a likelihood of : 0.997090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fine-Tuning Pre-Trained Neural Netowrks**\n",
        "* VGG (University of Oxford)\n",
        "* ResNet-50 (Microsoft Research)\n",
        "* Inception v3 (Google)\n",
        "* MobileNet (Google)\n",
        "* NASNet (Google)\n",
        "### **Two Uses**\n",
        "* Use a trained model directly to do image recognition\n",
        "* **Transfer Learning :** Adapt an existing model to recognize new types of objects instead of starting from scratch"
      ],
      "metadata": {
        "id": "uiijeN3TrZHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import vgg16"
      ],
      "metadata": {
        "id": "bbG7ITCirfOA"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = vgg16.VGG16()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZfylSa5stFh",
        "outputId": "72ac9d59-696f-42e0-d981-9aa9227223a8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 6s 0us/step\n",
            "553476096/553467096 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = image.load_img(\"bay.jpg\", target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "x = vgg16.preprocess_input(x)   # builtin normalizer in VGG"
      ],
      "metadata": {
        "id": "bUHVFzP6s4P1"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x)"
      ],
      "metadata": {
        "id": "lTetlac3tda9"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes = vgg16.decode_predictions(predictions, top=9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O3r2UPBtmO1",
        "outputId": "3445dc78-6bb5-4834-cdc2-79b78e0c67df"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "40960/35363 [==================================] - 0s 0us/step\n",
            "49152/35363 [=========================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top predictions in the image are : \")\n",
        "\n",
        "for imagenet_id, name, likelihood in predicted_classes[0]:\n",
        "    print(\"Prediction: {} - {:2f}\".format(name, likelihood))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkSDN7Q-t6SA",
        "outputId": "1caf7b05-977b-457f-e924-cf79460262f1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top predictions in the image are : \n",
            "Prediction: seashore - 0.395212\n",
            "Prediction: promontory - 0.326130\n",
            "Prediction: lakeside - 0.119613\n",
            "Prediction: breakwater - 0.062801\n",
            "Prediction: sandbar - 0.045267\n",
            "Prediction: cliff - 0.011845\n",
            "Prediction: dock - 0.009196\n",
            "Prediction: boathouse - 0.003278\n",
            "Prediction: valley - 0.003194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Transfer Learning**"
      ],
      "metadata": {
        "id": "v_p3jPJHuhFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dog_path = Path(\"training_folder\") / \"dogs\"\n",
        "not_dog_path = Path(\"training_folder\") / \"not_dogs\""
      ],
      "metadata": {
        "id": "t6Y2zeEXuT1B"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "labels = []"
      ],
      "metadata": {
        "id": "QSDimq34xxhw"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img in not_dog_path.glob(\"*.png\"):\n",
        "    # load the image\n",
        "    img = image.load_img(img)\n",
        "    # convert image to numpy array\n",
        "    image_array = image.img_to_array(img)\n",
        "    # add the image to the list\n",
        "    images.append(image_array)\n",
        "    labels.append(0)\n",
        "\n",
        "for img in dog_path.glob(\"*.png\"):\n",
        "    # load the image\n",
        "    img = image.load_img(img)\n",
        "    # convert image to numpy array\n",
        "    image_array = image.img_to_array(img)\n",
        "    # add the image to the list\n",
        "    images.append(image_array)\n",
        "    labels.append(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "ZMQ1RjvxxzIt",
        "outputId": "23a4d719-9cac-40e6-dac8-b57c92321504"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-98327ea444d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnot_dog_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# load the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# convert image to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    312\u001b[0m   \"\"\"\n\u001b[1;32m    313\u001b[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0;32m--> 314\u001b[0;31m                         target_size=target_size, interpolation=interpolation)\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2894\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m     raise UnidentifiedImageError(\n\u001b[0;32m-> 2896\u001b[0;31m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2897\u001b[0m     )\n\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7f345414b830>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6TdbUt7vylSb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}